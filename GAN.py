# -*- coding: utf-8 -*-
"""Lab8_GAN_for_image_generation_of_Cifar10 (2).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gl0JaBa1CsW9fj3E_rp_-ZQhFPcW6Ubx
"""

from google.colab import drive # Mount the google drive for data loading
drive.mount('/content/drive')

"""# Import some related dependencies
1.   Numpy: a package for array transformation
2.   Scipy: a package for loading data with (.mat) format
3.   Matplotlib: a package for data visualization
4.   Skearn: a package including many machine learning approaches
5.   Tensorflow: a package for neural networks modeling
6.   Keras: a package for neural networks modeling which is established on Tensorflow
"""

import numpy as np #helps for array operation
import scipy.io as sio #helps to read the data
import matplotlib.pyplot as plt #helps with graphical plots
from sklearn.preprocessing import OneHotEncoder #helps for label one-hot encoding
import tensorflow as tf #helps for ANN model construction
import keras #helps for ANN model construction
from keras.datasets import cifar10 #helps for loading cifar10 dataset

"""# Test the GPU"""

device_name = tf.test.gpu_device_name()  # Current GPU name
if device_name != '/device:GPU:0':       # Test if the GPU exists
  raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))

"""# Visualizaiton of raw images"""

(trainX,_), (_, _) = cifar10.load_data() # Download the Cifar10 data online. Only the training dataset will be used for GAN training.
#trainX = sio.loadmat('/content/drive/MyDrive/lab_code/Cifar10.mat')['trainX'] # Or you can load the Cifar10.mat data from local

trainX.shape

#trainX = trainX  	                                                        # Scale generated images from [-1,1] to [0,1]
ix = np.random.randint(0, trainX.shape[0], 7*7)				 		                    # Choose 7*7 random index
X = trainX[ix]		                                                            # Choose 7*7 samples as the selected index
plt.figure(figsize=(12,12))                                                   # Set the whole figure size
for i in range(7 * 7):           	                                            # Plot images
  plt.subplot(7, 7, 1 + i)                                                    # Define subplot
  plt.axis('off')									                                            # Turn off axis, only show the generated images
  plt.imshow(X[i]/255)                                                            # Show the images
plt.show()

"""#Define a discriminator
The discriminator is used to try to discriminate the generated samples and real samples.
"""

def define_discriminator(in_shape=(32,32,3)):   																	# A function with a parameter of input shape
	model = keras.models.Sequential()																								# Sequential model in keras to add layers
	model.add(keras.layers.Conv2D(64, (3,3), strides=(1,1),padding='same', input_shape=in_shape)) # Convolutional layer with 64 filters. Kernel size is (3,3), stride is (1,1)
	model.add(keras.layers.LeakyReLU(alpha=0.2))                                    # Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same'))       # Convolutional layer with 128 filters. Kernel size is (3,3), stride is (2,2)
	model.add(keras.layers.LeakyReLU(alpha=0.2))                                    # Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Conv2D(128, (3,3), strides=(2,2), padding='same'))       # Convolutional layer with 128 filters. Kernel size is (3,3), stride is (2,2)
	model.add(keras.layers.LeakyReLU(alpha=0.2))                                    # Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Conv2D(256, (3,3), strides=(2,2), padding='same'))       # Convolutional layer with 256 filters. Kernel size is (3,3), stride is (2,2)
	model.add(keras.layers.LeakyReLU(alpha=0.2))                                    # Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Flatten())             																	# Reshape the previous output to be a one-dimension vector
	model.add(keras.layers.Dropout(0.4))                                            # Alleviate overfitting with a dropout layer
	model.add(keras.layers.Dense(2, activation='softmax')) 													# Classifier. Real is [0, 1], fake is [1, 0]
	opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)                           # Set the learning rate of Adam optimizer
	model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])  # Compile the model with the optimizer, a loss of binary crossentropy, and an accuracy metric
	model.summary()																																	# Summary the discriminator model
	return model

"""#Define a generator
The generator is used to generate fake samples
"""

def define_generator(latent_dim):                                                   # A function with a parameter of input dim
	model = keras.models.Sequential() 																								# Sequential model in keras to add layers
	model.add(keras.layers.Dense(256 * 4 * 4, input_dim=latent_dim))                  # A fully connected layer to map the input_dim to a shape of 256*4*4
	model.add(keras.layers.LeakyReLU(alpha=0.2))																			# Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Reshape((4, 4, 256)))																			# Reshape the vector to be a 3-dimension image
	model.add(keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))# Transposed convolution layer with 128 filters. Kernel size is (4,4), stride is (2,2)
	model.add(keras.layers.LeakyReLU(alpha=0.2))																			# Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))# Transposed convolution layer with 128 filters. Kernel size is (4,4), stride is (2,2)
	model.add(keras.layers.LeakyReLU(alpha=0.2))																			# Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Conv2DTranspose(128, (4,4), strides=(2,2), padding='same'))# Transposed convolution layer with 128 filters. Kernel size is (4,4), stride is (2,2)
	model.add(keras.layers.LeakyReLU(alpha=0.2))																			# Leaky ReLU activation function with a slope of 0.2
	model.add(keras.layers.Conv2D(3, (3,3), strides=(1,1), activation='tanh', padding='same')) # Convolutional layer with 3 filters. Kernel size is (3,3), stride is (1,1), activation function is tanh (-1,1). The final output shape is 32x32x3, which is like an image.
	model.summary()																																		# Summary the generator model
	return model

"""#Define the GAN model based on discriminator and generator
The weights in discriminator are fixed, and GAN model only updates the weights in generator.
"""

def define_gan(g_model, d_model):												   # A function with two parameters: discriminator model and generator model
	d_model.trainable = False                                # Make weights in the discriminator not trainable
	model = keras.models.Sequential()                        # Sequential model in keras to add the two models
	model.add(g_model)                                       # Add generator
	model.add(d_model)																			 # Add the discriminator
	model.summary()																					 # Summary the GAN model
	opt = tf.keras.optimizers.Adam(lr=0.0002, beta_1=0.5)    # Set the learning rate of Adam optimizer
	model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy']) # Compile the model with the optimizer, a loss of binary crossentropy, and an accuracy metric
	return model

"""# Load the real images

1.   Import the Cifar10.mat data
2.   Take one sample every ten samples
3.   Convert data from unsigned ints to floats
4.   Normalize the data [0, 255] to an interval of [-1, 1]
"""

def load_real_samples():                                                              # Define a function of loading real samples
	X = trainX[::10]																														# Load the Cifar10.mat data and take one sample every ten samples
	X = X.astype('float32')                                                        # Convert data from unsigned ints to floats
	X = (X - 127.5) / 127.5                               															# Scale data from [0,255] to [-1,1]
	return X

"""# Select real images and define labels"""

def generate_real_samples(dataset, n_samples):                  	   # A function with two parameters: data and the number of samples in a batch
	ix = np.random.randint(0, dataset.shape[0], n_samples)				 		 # Choose n_samples random instances
	X = dataset[ix]
	y = np.repeat(np.asarray([0, 1]).reshape(1, 2), n_samples, axis=0) # Define the labels of real images using [0, 1]
	return X, y

"""# Generate input for the generator"""

def generate_latent_points(latent_dim, n_samples):    # A function with two parameters: input dimension and the number of generated samples in a batch
	x_input = np.random.randn(latent_dim * n_samples) 	# Generate random noise vector with a length of latent_dim * n_samples
	x_input = x_input.reshape(n_samples, latent_dim)    # Reshape into a batch of inputs for the generator
	return x_input

"""# Generate fake samples using the generator"""

def generate_fake_samples(g_model, latent_dim, n_samples):           # A function with three parameters: generator model, input dimension and the number of generated samples in a batch
	x_input = generate_latent_points(latent_dim, n_samples)            # Generate input for the generator
	X = g_model.predict(x_input)																	 	   # predict outputs, i.e., generate fake samples
	y = np.repeat(np.asarray([1, 0]).reshape(1, 2), n_samples, axis=0) # Define the labels of fake images using [1, 0]
	return X, y

"""# Display the generated images"""

def plot_generate(examples, n = 7): # A function with three parameters: generated images, the number of images n^2 once displayed. Default n is 7, you can change it.
	plt.figure(figsize=(12,12))       # Set the whole figure size
	examples = (examples + 1) / 2.0  	# Scale generated images from [-1,1] to [0,1]
	for i in range(n * n):           	# Plot images
		plt.subplot(n, n, 1 + i)        # Define subplot
		plt.axis('off')									# Turn off axis, only show the generated images
		plt.imshow(examples[i])         # Show the images
	plt.show()
	plt.close()

"""# Evaluate the discriminator


1.   Evaluate the classification accuracy for both real samples and fakes samples
2.   Save generator model



"""

def summarize_performance(epoch, g_model, d_model, dataset, latent_dim, n_samples=128):  # A function with six parameters: current training epoch, current generator model, current discriminator model, real dataset, input dimension for generator, the number of samples in a batch (default 128)
	X_real, y_real = generate_real_samples(dataset, n_samples)                      	# Prepare real samples
	_, acc_real = d_model.evaluate(X_real, y_real, verbose=0)													# Evaluate discriminator on real examples
	x_fake, y_fake = generate_fake_samples(g_model, latent_dim, n_samples)						# Prepare fake examples
	_, acc_fake = d_model.evaluate(x_fake, y_fake, verbose=0)													# Evaluate discriminator on fake examples
	print(f'>Accuracy real: {acc_real*100}%, fake: {acc_fake*100}%')      						# Summarize discriminator performance
	plot_generate(x_fake)																															# Plot the generated images
	filename = 'generator_model_%d.h5' % (epoch+1)																	  # Save the generator model
	g_model.save(filename)

"""# Train the generator and discriminator


1.   Train discriminator using a half batch of real images
2.   Train discriminator using a half batch of fake images
3.   Generate a batch of fake samples and define their labels as real images (i.e., [0, 1])
4.   Train generator using a batch of fake images


"""

def train(g_model, d_model, gan_model, dataset, latent_dim, n_epochs=500, n_batch=128): # Define the training model, n_epochs = 500, batch size = 128.
	bat_per_epo = int(dataset.shape[0] / n_batch)                                         # Calculate the number of batches in this cifar dataset
	half_batch = int(n_batch / 2)																													# Compute a half of batch
	for i in range(n_epochs): 																														# A loop of model training for n_epochs
		for j in range(bat_per_epo):																												# A loop of model training for bat_per_epo times in each epoch
			X_real, y_real = generate_real_samples(dataset, half_batch)												# Get randomly selected 'real' images of half batch
			d_loss1, _ = d_model.train_on_batch(X_real, y_real)																# Train discriminator model weights using real images
			X_fake, y_fake = generate_fake_samples(g_model, latent_dim, half_batch)						# Generate 'fake' images of half batch
			d_loss2, _ = d_model.train_on_batch(X_fake, y_fake)														  	# Train discriminator model weights using fake images
			X_gan = generate_latent_points(latent_dim, n_batch)																# Prepare input of a batch for the generator
			y_gan = np.repeat(np.asarray([0,1]).reshape(1,2), n_batch, axis=0)						  	# Define their labels as real images (i.e., [0, 1]) to generate more realistic images
			g_loss, _ = gan_model.train_on_batch(X_gan, y_gan)																# Train generator model weights using fake images
			print('>%d, %d/%d, d1=%.3f, d2=%.3f g=%.3f' % (i+1, j+1, bat_per_epo, d_loss1, d_loss2, g_loss))  			# Summarize loss on this batch
		if (i+1) % 10 == 0:																																	# Evaluate the model performance every 10 epochs
			summarize_performance(i, g_model, d_model, dataset, latent_dim)

"""# Model training and visualization"""

dataset = load_real_samples()                        # Load real images
dataset.shape

latent_dim = 100                                     # Give the input length of the generator model
d_model = define_discriminator()                     # Create the discriminator
g_model = define_generator(latent_dim)               # Create the generator
gan_model = define_gan(g_model, d_model)             # Create the gan
dataset = load_real_samples()                        # Load real images
train(g_model, d_model, gan_model, dataset, latent_dim) # Train model using the parameters

"""# Generate some fake images using the Generator"""

model = keras.models.load_model('generator_model_490.h5') # Load model
latent_points = generate_latent_points(100, 50)           # Generate input of 50 samples for generator
X = model.predict(latent_points)                          # Generate fake images
plot_generate(X, 7)                                       # Plot the results

